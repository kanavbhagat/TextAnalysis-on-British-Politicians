{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cdf688e",
   "metadata": {},
   "source": [
    "# Sentiment and Thematic Analyses of British Politician Speeches\n",
    "\n",
    "#### Kanav Bhagat 730013149"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a4388",
   "metadata": {},
   "source": [
    "## importaing important libraries that will be used for the experimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9229713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import gensim\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3d930",
   "metadata": {},
   "source": [
    "## Downloading important modules from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacfcd5",
   "metadata": {},
   "source": [
    "## Loading the Dataset into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Dataset into datafram\n",
    "Speech_data = pd.read_csv('SpeechData_with_Party.csv')\n",
    "print(Speech_data.head())\n",
    "\n",
    "'''\n",
    "# Counting the number of speeches by Conservative, Labour, and Other parties\n",
    "speech_count_by_party = Speech_data['Party'].value_counts()\n",
    "print(\"Number of speeches by each party:\\n\", speech_count_by_party)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2193c49",
   "metadata": {},
   "source": [
    "### Speeches Dataset Overview\n",
    "\n",
    "Below is a preview of the dataset containing speeches from British politicians, categorized by their name, title, URL, text, and party affiliation.\n",
    "\n",
    "| Politician Name | Speech Title                                             | Speech URL                                                        | Party        |\n",
    "|-----------------|----------------------------------------------------------|-------------------------------------------------------------------|--------------|\n",
    "| Scott Benton    | 2024 Resignation Statement                               | [Link](https://www.ukpol.co.uk/scott-benton-2024-resignation...)  | Other        |\n",
    "| Rishi Sunak     | 2024 Speech on Extremism                                 | [Link](https://www.ukpol.co.uk/rishi-sunak-2024-speech-on-ext...)  | Conservative |\n",
    "| Michael Gove    | 2024 Speech at the Convention of the North               | [Link](https://www.ukpol.co.uk/michael-gove-2024-speech-at-th...)  | Conservative |\n",
    "| Oliver Dowden   | 2024 Speech on AI for Public Good                        | [Link](https://www.ukpol.co.uk/oliver-dowden-2024-speech-on-a...)  | Conservative |\n",
    "| Stuart Andrew   | 2024 Speech at the Beacon Philanthropy and Impact Forum  | [Link](https://www.ukpol.co.uk/stuart-andrew-2024-speech-at-t...)  | Conservative |\n",
    "\n",
    "This table provides a quick look at the diversity of topics and party representation in our dataset. Each speech's URL is linked for easy access to the full text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db1187",
   "metadata": {},
   "source": [
    "## Performing Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c9a3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the sentiment Analyser \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    sentiment_score = sia.polarity_scores(text)\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630c5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the Sentiment Analysis over the speeches. \n",
    "#No need to apply any pre-processing since Vader is efficient to work over human speeches. \n",
    "sentiment_df = Speech_data['Speech Text'].apply(analyze_sentiment_vader)\n",
    "Speech_data[['Neg', 'Neu', 'Pos', 'Compound']] = pd.json_normalize(sentiment_df) #Adding new columns to the dataframe for generating results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Average compound of all the speeches\n",
    "average_compound = Speech_data['Compound'].mean()\n",
    "print(f\"Average Compound Sentiment Score: {average_compound:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9d18f",
   "metadata": {},
   "source": [
    "### Average Compound Sentiment Score\n",
    "\n",
    "The overall average compound sentiment score across all analyzed speeches is **0.808**. This score indicates a generally positive sentiment within the political speeches in our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3606f50",
   "metadata": {},
   "source": [
    "### Analysing the data using the graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Sentiment distribution across all the speeches.\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.histplot(Speech_data['Compound'], bins=30, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Compound Sentiment Scores Across All Speeches')\n",
    "plt.xlabel('Compound Score')\n",
    "plt.ylabel('Number of Speeches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6cd3f5",
   "metadata": {},
   "source": [
    "#### Graph representing Distribution of Compound Sentiment Scores Across All Speeches \n",
    "![Graph Results](Markdown%20Images/graph-1.png \"Graph showing Distribution of Compound Sentiment Scores Across All Speeches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average sentiment scores for each political party\n",
    "avg_sentiment_by_party = Speech_data.groupby('Party')[['Compound']].mean().reset_index()\n",
    "\n",
    "print(avg_sentiment_by_party)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Party', y='Compound', data=avg_sentiment_by_party, palette='Purples')\n",
    "plt.title('Average Compound Sentiment')\n",
    "plt.xlabel('Party')\n",
    "plt.ylabel('Average Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc2ca4",
   "metadata": {},
   "source": [
    "### Results for sentiments based upon different parties\n",
    "\n",
    "Below is the summary of average compound sentiment scores by party:\n",
    "\n",
    "| Party         | Compound |\n",
    "|---------------|----------|\n",
    "| Conservative  | 0.884162 |\n",
    "| Labour        | 0.467271 |\n",
    "| Other         | 0.290737 |\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "For a more detailed analysis, refer to the graph below:\n",
    "\n",
    "![Sentiment Analysis Results](Markdown%20Images/graph-2.png \"Sentiment Analysis Graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e903f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying speeches with the highest and lowest compound scores\n",
    "highest_sentiment_speech = Speech_data.loc[Speech_data['Compound'].idxmax()]\n",
    "lowest_sentiment_speech = Speech_data.loc[Speech_data['Compound'].idxmin()]\n",
    "\n",
    "print(f\"Highest Sentiment Speech: '{highest_sentiment_speech['Speech Title']}' by {highest_sentiment_speech['Politician Name']} from {highest_sentiment_speech['Party']} (Compound: {highest_sentiment_speech['Compound']})\")\n",
    "print(f\"Lowest Sentiment Speech: '{lowest_sentiment_speech['Speech Title']}' by {lowest_sentiment_speech['Politician Name']} from {highest_sentiment_speech['Party']} (Compound: {lowest_sentiment_speech['Compound']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a26fc",
   "metadata": {},
   "source": [
    "### Speech Sentiment Extremes\n",
    "\n",
    "#### Highest Sentiment Speech\n",
    "- **Title:** 2024 Budget Speech\n",
    "- **Politician:** Jeremy Hunt\n",
    "- **Party:** Conservative\n",
    "- **Sentiment Score (Compound):** 1.0\n",
    "\n",
    "#### Lowest Sentiment Speech\n",
    "- **Title:** 2024 Speech on Freedom and Democracy in Iran\n",
    "- **Politician:** Bob Blackman\n",
    "- **Party:** Conservative\n",
    "- **Sentiment Score (Compound):** -0.9998\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc582844",
   "metadata": {},
   "source": [
    "## Performing Themetic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0778727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Funtion preprocess_text pre-processes the text data by various different ways.\n",
    "\"\"\"\n",
    "- It lowercase the text, \n",
    "- Remove the first sentence since first sentence of each speech tells the name of politician, title of speech and date it was delivered.\n",
    "- Tokenize the text\n",
    "- Remove non-alphanumeric characters and stopwords. Added a list of common stopwords in english to get us the most refined themes. \n",
    "- Lemmatize the text\n",
    "- Generate bigrams and multi-word texts \n",
    "\n",
    "\"\"\"\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    entities = [entity.text for entity in doc.ents]\n",
    "    # Combine entity tokens back into the text\n",
    "    text_with_entities = ' '.join(entities) + ' ' + text\n",
    "    \n",
    "    # Remove the first sentence (usually metadata in political speeches)\n",
    "    sentences = nltk.sent_tokenize(text_with_entities)\n",
    "    if len(sentences) > 1:\n",
    "        text = ' '.join(sentences[1:])\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters and stopwords\n",
    "    common_stop_words = {\"also\",\"new\",\"year\",\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",}\n",
    "    stop_words = set(stopwords.words('english')).union(common_stop_words)\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Generate and add bigrams to capture multi-word themes\n",
    "    bigrams = ['_'.join(gram) for gram in ngrams(tokens, 2)]\n",
    "    tokens.extend(bigrams)\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358adb2e",
   "metadata": {},
   "source": [
    "### Preprocessing the speeches based upon the political party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b9a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-process the speeches based upon the political party\n",
    "Speech_data['Titile and Speech'] = Speech_data['Speech Title'] + \". \" + Speech_data['Speech Text']\n",
    "\n",
    "pre_processed_conservative= Speech_data[Speech_data['Party'] == 'Conservative']['Titile and Speech'].apply(preprocess_text)\n",
    "pre_processed_labour= Speech_data[Speech_data['Party'] == 'Labour']['Titile and Speech'].apply(preprocess_text)\n",
    "pre_processed_other= Speech_data[Speech_data['Party'] == 'Other']['Titile and Speech'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645432c",
   "metadata": {},
   "source": [
    "### Function to process texts before we can apply LDA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5ee507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to process texts before we can apply LDA. \n",
    "#This function builds the dictionary and a corpus from the speech data to be used for LDA\n",
    "def process_speeches(preprocessed_speeches):\n",
    "    # Tokenize the preprocessed speeches\n",
    "    # Initialize an empty list to hold the filtered tokens\n",
    "    texts = []\n",
    "\n",
    "    for speech in preprocessed_speeches:\n",
    "        # Tokenize the preprocessed speech\n",
    "        tokens = word_tokenize(speech)\n",
    "        \n",
    "        # Perform POS tagging on the tokens\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        \n",
    "        # Filter tokens based on POS tags (keeping only nouns and verbs as an example)\n",
    "        allowed_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "        important_words = [word for word, tag in tagged_tokens if tag in allowed_pos_tags]\n",
    "        \n",
    "        texts.append(important_words)\n",
    "\n",
    "    # Create a dictionary representation of the documents, and filter extremes\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
    "\n",
    "    # Convert document into the bag-of-words (BoW) format\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    if len(corpus) == 0 or len(dictionary) == 0:\n",
    "        print(\"Warning: Corpus or dictionary is empty. Consider adjusting filtering criteria.\")\n",
    "    \n",
    "    \n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3834e9",
   "metadata": {},
   "source": [
    "### LDA function to perform themetic Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa11851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create LDA model with corpus and dictionary. We will generate 5 topics for each party corpus.\n",
    "def create_lda_model(corpus, dictionary, num_topics=5, passes=15):\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        chunksize=100,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=400,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes\n",
    "    )\n",
    "\n",
    "    # Get topics from the model\n",
    "    topics = model.print_topics(num_words=4)\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91434e66",
   "metadata": {},
   "source": [
    "### Getting the themes for different parties using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the LDA to our speeches filtered upon parties. \n",
    "\n",
    "corpus_conservative, dictionary_conservative, = process_speeches(pre_processed_conservative)\n",
    "Keywords_conservative = create_lda_model(corpus_conservative, dictionary_conservative, num_topics=5)\n",
    "\n",
    "corpus_labour, dictionary_labour, = process_speeches(pre_processed_labour)\n",
    "Keywords_labour = create_lda_model(corpus_labour, dictionary_labour, num_topics=5)\n",
    "\n",
    "corpus_other, dictionary_other, = process_speeches(pre_processed_other)\n",
    "Keywords_other = create_lda_model(corpus_other, dictionary_other, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa08245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the results\n",
    "print(\"Conservative\")\n",
    "for i in Keywords_conservative:\n",
    "    print(i)\n",
    "\n",
    "print(\"Labour\")\n",
    "for i in Keywords_labour:\n",
    "    print(i)\n",
    "\n",
    "print(\"Other\")\n",
    "for i in Keywords_other:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5d9c8",
   "metadata": {},
   "source": [
    "## Thematic Analysis Results\n",
    "\n",
    "### Conservative Themes\n",
    "1. `0.012*\"people\" + 0.010*\"investment\" + 0.010*\"business\" + 0.009*\"work\"`\n",
    "2. `0.010*\"ireland\" + 0.007*\"northern_ireland\" + 0.007*\"government\" + 0.007*\"defence\"`\n",
    "3. `0.014*\"gambling\" + 0.008*\"government\" + 0.008*\"ireland\" + 0.008*\"consultation\"`\n",
    "4. `0.016*\"woman\" + 0.013*\"government\" + 0.012*\"people\" + 0.009*\"service\"`\n",
    "5. `0.013*\"food\" + 0.009*\"country\" + 0.008*\"child\" + 0.008*\"today\"`\n",
    "\n",
    "### Labour Themes\n",
    "1. `0.085*\"government\" + 0.055*\"authority\" + 0.047*\"funding\" + 0.036*\"care\"`\n",
    "2. `0.060*\"people\" + 0.058*\"politics\" + 0.046*\"country\" + 0.042*\"britain\"`\n",
    "3. `0.077*\"ireland\" + 0.060*\"northern_ireland\" + 0.042*\"minister\" + 0.030*\"market\"`\n",
    "4. `0.039*\"member\" + 0.029*\"government\" + 0.029*\"hon_member\" + 0.026*\"house\"`\n",
    "5. `0.005*\"house\" + 0.005*\"member\" + 0.005*\"government\" + 0.005*\"hope\"`\n",
    "\n",
    "### Other Party Themes\n",
    "1. `0.045*\"continue\" + 0.045*\"return\" + 0.045*\"comment\" + 0.024*\"life\"`\n",
    "2. `0.051*\"community\" + 0.051*\"mp\" + 0.041*\"government\" + 0.041*\"country\"`\n",
    "3. `0.064*\"house\" + 0.045*\"party\" + 0.039*\"opposition\" + 0.039*\"leader\"`\n",
    "4. `0.064*\"office\" + 0.049*\"meet\" + 0.049*\"minister\" + 0.034*\"court\"`\n",
    "5. `0.075*\"iran\" + 0.034*\"freedom\" + 0.021*\"people\" + 0.021*\"member\"`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
